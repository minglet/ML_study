{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.90066591  1.63807606 -0.84538405]\n",
      " [ 1.30965265 -0.63327602  1.44838936]\n",
      " [-0.29903844 -0.38673405 -0.84598899]\n",
      " [ 0.29202567  0.83916396 -0.98961364]\n",
      " [ 0.1370709   2.31409889 -0.72664352]\n",
      " [-0.6505606   1.37691074  1.17466226]\n",
      " [-0.54524173 -0.06539849  1.81200707]]\n",
      "[[-1.90066591  1.63807606 -0.84538405]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # one-hot encoding\n",
    "W = np.random.randn(7, 3) # random한 weights 구함\n",
    "print(W)\n",
    "h = np.matmul(c, W) # W_in과 c(input word)를 matmul 시켜서 행뽑기를 하는 것임\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.42292931  1.51499091 -0.1182692 ]\n",
      " [ 0.29217769  0.48745789  0.50117567]\n",
      " [-1.49255114 -0.38795633  0.39407587]\n",
      " [-1.77329012  0.98185021 -0.15736354]\n",
      " [ 0.39865025  0.54464587 -1.1002162 ]\n",
      " [-0.76040161 -0.84158808 -1.02621694]\n",
      " [-0.83752641  1.68234119 -0.72969668]]\n",
      "[[ 1.42292931  1.51499091 -0.1182692 ]]\n"
     ]
    }
   ],
   "source": [
    "# common.layer(만든 함수)에 Matmul 함수를 넣어서 똑같은 결과를 봅시다\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Matmul\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "W = np.random.randn(7, 3)\n",
    "print(W)\n",
    "layer = Matmul(W)\n",
    "h = layer.forward(c) # weight와 연산 할 x를 넣어주는 것임\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.59836701  1.34803703 -1.44426032  2.02358586 -1.91176526 -2.58540393\n",
      "  -2.1412246 ]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Matmul\n",
    "\n",
    "# 샘플 context 데이터\n",
    "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "c1 = np.array([[0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "# 가중치 초기화\n",
    "# 두개의 weight는 tranfer된 상태의 행렬이 아님!!\n",
    "W_in = np.random.randn(7, 3) \n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "# 계층 생성\n",
    "in_layer0 = Matmul(W_in) # input(context)가 두개가 들어오기 때문\n",
    "in_layer1 = Matmul(W_in)\n",
    "out_layer = Matmul(W_out)\n",
    "\n",
    "# forward\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1) # contexts가 모여서 평균 내는 구간\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "# 어떻게 동작하는지를 알았으니, 이제 실제 text를 넣어서 학습시켜보기\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''\n",
    "    context, target 생성\n",
    "    corpus: 말뭉치 (단어 ID 목록)\n",
    "    window_size: 윈도우 크기\n",
    "    return (context, target)의 array\n",
    "    '''\n",
    "\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size): \n",
    "        cs = []\n",
    "        # window 크기만큼 타겟 단어 좌우 context 가져오기 \n",
    "        # [-1, 0, 1] 이렇게 배치되어있을 때 0은 target, -1, 1은 contexts를 의미함\n",
    "        for t in range(-window_size, window_size+1):\n",
    "            if t != 0:\n",
    "                cs.append(corpus[idx + t]) # context인 경우 cs에 추가 / idx의 의미는 해당 corpus[idx]가 target이라는 말\n",
    "        contexts.append(cs)\n",
    "    \n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "# from common.util import create_contexts_target\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''\n",
    "    원핫 표현으로 변환\n",
    "    corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    vocab_size: 어휘 수\n",
    "    return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "# from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "text = 'You say goodby and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]]\n"
     ]
    }
   ],
   "source": [
    "print(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Matmul, SoftmaxWithLoss\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f') # astype('f'): 부동소수점 형식\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 레이어 생성\n",
    "        self.in_layer0 = Matmul(W_in)\n",
    "        self.in_layer1 = Matmul(W_in)\n",
    "        self.out_layer = Matmul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모으기\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현 저장 # 무슨 용도지?\n",
    "        self.word_vecs1 = W_in\n",
    "        self.word_vecs2 = W_out.T\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts)\n",
    "        h1 = self.in_layer1.forward(contexts)\n",
    "        h = 0.5 * (h0 + h1)\n",
    "        print(\"h\", h.shape)\n",
    "        score = self.out_layer.forward(h)\n",
    "        print(score.shape, target.shape)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout = 1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2, 7) (6, 7)\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('..')\n",
    "from common.trainer import Trainer\n",
    "from common.optimization import Adam\n",
    "# from simple_cbow import SimpleCBOW\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "# cbow 학습 데이터셋 생성\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "# Input에 맞는 one-hot 표현 변환\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "# contexts.shape -> (6,2,7) context 벡터 크기 (6,7). target 주변의 context 벡터 2개, \n",
    "print(contexts.shape, target.shape)\n",
    "# 모델 초기화\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h (3, 2, 5)\n",
      "(3, 2, 7) (3, 7)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (3,) (3,7) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/minah/git/ML_study/deep_learning/ch03-Word2Vec/../common/trainer.py:38\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m     35\u001b[0m batch_t \u001b[38;5;241m=\u001b[39m t[iters\u001b[38;5;241m*\u001b[39mbatch_size:(iters\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 기울기 구해 매개변수 갱신\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m params, grads \u001b[38;5;241m=\u001b[39m remove_duplicate(model\u001b[38;5;241m.\u001b[39mparams, model\u001b[38;5;241m.\u001b[39mgrads)  \u001b[38;5;66;03m# 공유된 가중치를 하나로 모음\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 38\u001b[0m, in \u001b[0;36mSimpleCBOW.forward\u001b[0;34m(self, contexts, target)\u001b[0m\n\u001b[1;32m     36\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer\u001b[38;5;241m.\u001b[39mforward(h)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(score\u001b[38;5;241m.\u001b[39mshape, target\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/minah/git/ML_study/deep_learning/ch03-Word2Vec/../common/layers.py:80\u001b[0m, in \u001b[0;36mSoftmaxWithLoss.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy_error\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/minah/git/ML_study/deep_learning/ch03-Word2Vec/../common/functions.py:33\u001b[0m, in \u001b[0;36mcross_entropy_error\u001b[0;34m(y, t)\u001b[0m\n\u001b[1;32m     29\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 예측한 것들 중 가장 높은 즉 확률이 가장 높은 값의 인덱스를 출력\u001b[39;00m\n\u001b[1;32m     31\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 33\u001b[0m cross_entropy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-7\u001b[39m)\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(cross_entropy) \u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (3,) (3,7) "
     ]
    }
   ],
   "source": [
    "trainer.fit(contexts, target, max_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
