{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 모델 : KNeighbor, RandomForest, Adaboost, DecisionTree, Logistic(meta model C=10)\n",
    "# 사용할 데이터 : cancer data\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model 선언\n",
    "kn_clf = KNeighborsClassifier(n_neighbors=4)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# meta model 선언\n",
    "lg_clf = LogisticRegression(C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114,)\n",
      "(114,)\n",
      "(114,)\n",
      "(114,)\n"
     ]
    }
   ],
   "source": [
    "# 개별 모델 학습\n",
    "kn_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# 개별 모델 예측\n",
    "kn_pred = kn_clf.predict(X_test)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "ada_pred = ada_clf.predict(X_test)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "\n",
    "# 개별 모델 예측 결과 shape: (114, ) --> 최종 메타 학습 데이터 shape: (114, 4)\n",
    "print(kn_pred.shape)\n",
    "print(rf_pred.shape)\n",
    "print(ada_pred.shape)\n",
    "print(dt_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 114)\n",
      "(114, 4)\n"
     ]
    }
   ],
   "source": [
    "# 개별 모델 예측 결과 array로 합치기\n",
    "preds = np.array([kn_pred, rf_pred, ada_pred, dt_pred])\n",
    "print(preds.shape)\n",
    "\n",
    "# (114, 4)로 만들기 위해서 transpose 취해주기\n",
    "meta_X_train = np.transpose(preds)\n",
    "print(meta_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kn_clf accuaracy: 0.939\n",
      "rf_clf accuaracy: 0.965\n",
      "ada_clf accuaracy: 0.974\n",
      "dt_clf accuaracy: 0.939\n"
     ]
    }
   ],
   "source": [
    "# 개별 모델의 예측 성능 평가\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('kn_clf accuaracy: {:.3f}'.format(accuracy_score(kn_pred, y_test)))\n",
    "print('rf_clf accuaracy: {:.3f}'.format(accuracy_score(rf_pred, y_test)))\n",
    "print('ada_clf accuaracy: {:.3f}'.format(accuracy_score(ada_pred, y_test)))\n",
    "print('dt_clf accuaracy: {:.3f}'.format(accuracy_score(dt_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta model accuaracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "# 메타 model 적용하기\n",
    "\n",
    "lg_clf.fit(meta_X_train, y_test) # 여기서 meta_X_train은 사실 X_test 데이터를 보고 예측한 것이기 때문에 테스트셋은 이미 오염되었다.\n",
    "# y_test가 메타 모델의 라벨인 이유는 개별 모델의 X_test 데이터를 predict로 예측한 것의 정답 레이블이기 때문이다.\n",
    "meta_pred = lg_clf.predict(meta_X_train)\n",
    "print('meta model accuaracy: {:.3f}'.format(accuracy_score(meta_pred, y_test)))\n",
    "# 당연히 과적합이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 1)\n",
      "(114, 3)\n",
      "## 0 fold start ##\n",
      "## 1 fold start ##\n",
      "## 2 fold start ##\n",
      "평균 전 shape: (114, 3) --> 평균 후 shape: (114,)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation을 이용해 stacking 구현하기 (KFold 사용)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=False)\n",
    "\n",
    "# 0으로 모델 trainset, testset 초기화하기\n",
    "train_data = np.zeros((X_train.shape[0], 1)) # shape (M, 1)\n",
    "print(train_data.shape)\n",
    "test_data = np.zeros((X_test.shape[0], 3)) # shape (K, kold number) --> 이후 평균 내는 것\n",
    "print(test_data.shape)\n",
    "\n",
    "for iter, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    X_tr, y_tr = X_train[train_idx], y_train[train_idx]\n",
    "    val_train = X_train[val_idx]\n",
    "    \n",
    "    print(\"## {} fold start ##\".format(iter))\n",
    "    kn_clf.fit(X_tr, y_tr)\n",
    "    pred = kn_clf.predict(val_train)\n",
    "    train_data[val_idx, :] = pred.reshape(-1, 1)\n",
    "    test_data[:, iter] = kn_clf.predict(X_test)\n",
    "    \n",
    "# test data 평균내기\n",
    "meta_test = np.mean(test_data, axis=1)\n",
    "print('평균 전 shape: {0} --> 평균 후 shape: {1}'.format(test_data.shape, meta_test.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 1)\n",
      "(6, 1)\n",
      "[[0.5]\n",
      " [0.8]\n",
      " [0.6]\n",
      " [0.4]\n",
      " [0.5]\n",
      " [0.3]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]\n",
      " [0. ]]\n"
     ]
    }
   ],
   "source": [
    "# train set을 폴드 별로 나눠서 pred 값 추가하는 것 연습해보기\n",
    "\n",
    "zeros = np.zeros((X_train.shape[0], 1)) # shape (M, 1)\n",
    "print(zeros.shape)\n",
    "index = [0,1,2,3,4,5]\n",
    "pred_ex = np.array([0.5, 0.8, 0.6, 0.4, 0.5, 0.3]).reshape(-1,1)\n",
    "print(pred_ex.shape)\n",
    "zeros[index, :] = pred_ex\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation으로 stacking 함수로 만들기\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "def get_stacking_train_test(model, n_split: int, X_train, X_test, y_train):\n",
    "    kf = KFold(n_splits=n_split)\n",
    "    # train, test dataset 0으로 초기화 하기\n",
    "    # trainset shape: (M, 1), testset shape: (K, fold 개수)\n",
    "    train_data = np.zeros((X_train.shape[0], 1))\n",
    "    test_data = np.zeros((X_test.shape[0], n_split))\n",
    "    \n",
    "    print('## {} train start'.format(model.__class__.__name__))\n",
    "    # KFold를 이용하여 학습 및 검증\n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        # index에 따른 train, val set 재지정\n",
    "        X_tr, y_tr, X_val = X_train[train_idx], y_train[train_idx], X_train[val_idx]\n",
    "        \n",
    "        # 학습 \n",
    "        print(\"## {} fold train start\".format(fold_num))\n",
    "        model.fit(X_tr, y_tr)\n",
    "        pred = model.predict(X_val)\n",
    "        # testdata에 대해서 학습\n",
    "        pred_test = model.predict(X_test)\n",
    "        \n",
    "        # 초기화했던 것에 값 넣기\n",
    "        train_data[val_idx, :] = pred.reshape(-1,1) # 길게 한줄로 만들어서 넣기 reshape(-1,1)\n",
    "        test_data[:, fold_num] = pred_test\n",
    "        \n",
    "    # testset 평균 내서 한 줄로 만들기\n",
    "    test_data = np.mean(test_data, axis=1).reshape(-1, 1)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## KNeighborsClassifier train start\n",
      "## 0 fold train start\n",
      "## 1 fold train start\n",
      "## 2 fold train start\n",
      "## 3 fold train start\n",
      "## 4 fold train start\n",
      "## 5 fold train start\n",
      "## 6 fold train start\n",
      "## RandomForestClassifier train start\n",
      "## 0 fold train start\n",
      "## 1 fold train start\n",
      "## 2 fold train start\n",
      "## 3 fold train start\n",
      "## 4 fold train start\n",
      "## 5 fold train start\n",
      "## 6 fold train start\n",
      "## AdaBoostClassifier train start\n",
      "## 0 fold train start\n",
      "## 1 fold train start\n",
      "## 2 fold train start\n",
      "## 3 fold train start\n",
      "## 4 fold train start\n",
      "## 5 fold train start\n",
      "## 6 fold train start\n",
      "## DecisionTreeClassifier train start\n",
      "## 0 fold train start\n",
      "## 1 fold train start\n",
      "## 2 fold train start\n",
      "## 3 fold train start\n",
      "## 4 fold train start\n",
      "## 5 fold train start\n",
      "## 6 fold train start\n",
      "(455, 4)\n",
      "(114, 4)\n"
     ]
    }
   ],
   "source": [
    "# 모델별로 개별 train & test dataset 뽑아내기\n",
    "kn_train, kn_test = get_stacking_train_test(kn_clf, 7, X_train, X_test, y_train)\n",
    "rf_train, rf_test = get_stacking_train_test(rf_clf, 7, X_train, X_test, y_train)\n",
    "ada_train, ada_test = get_stacking_train_test(ada_clf, 7, X_train, X_test, y_train)\n",
    "dt_train, dt_test = get_stacking_train_test(dt_clf, 7, X_train, X_test, y_train)\n",
    "\n",
    "# 메타 모델을 위한 train, test dataset 만들기\n",
    "# 시도했지만 실패한 코드 아래의 예시의 경우 dimension을 추가해서 합하는 경우이다.\n",
    "# meta_train = np.array([kn_train, rf_train, ada_train, dt_train]) # shape: (4, 455, 1)\n",
    "# meta_test = np.array([kn_test, rf_test, ada_test, dt_test]) # shape: (4, 114, 1)\n",
    "\n",
    "meta_train = np.concatenate((kn_train, rf_train, ada_train, dt_train), axis=1)\n",
    "meta_test = np.concatenate((kn_test, rf_test, ada_test, dt_test), axis=1)\n",
    "print(meta_train.shape)\n",
    "print(meta_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타 모델을 위한 trainset shape: (455, 4), testset shape: (114, 4)\n",
      "final accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "# 메타 모델 학습 시키기\n",
    "# 메타 모델 : Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_clf = LogisticRegression(C=10)\n",
    "lr_clf.fit(meta_train, y_train)\n",
    "meta_pred = lr_clf.predict(meta_test)\n",
    "print('메타 모델을 위한 trainset shape: {0}, testset shape: {1}'.format(meta_train.shape, meta_test.shape))\n",
    "print('final accuracy: {:.3f}'.format(accuracy_score(meta_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
